from __future__ import annotations

import argparse
import sys
from pathlib import Path

import pandas as pd
import numpy as np

repo_root = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(repo_root))

from src.data_eng.config import load_config
from src.data_eng.io import partition_ref

def check_integrity_v2(dt: str) -> None:
    cfg = load_config(repo_root=repo_root, config_path=repo_root / "src/data_eng/config/datasets.yaml")
    
    # 1. Resolve symbol
    selection_path = repo_root / "lake" / "selection" / "mbo_contract_day_selection.parquet"
    if not selection_path.exists():
        raise FileNotFoundError(f"Missing selection map: {selection_path}")
    df_sel = pd.read_parquet(selection_path)
    row = df_sel.loc[df_sel["session_date"].astype(str) == dt]
    if row.empty:
        # Fallback for hardcoded verification date if not in selection
        symbol = "ESH6" 
    else:
        symbol = str(row.iloc[0]["selected_symbol"]).strip()
    
    print(f"Checking integrity (V2) for {symbol} on {dt}")

    # =========================================================================
    # GEX SURFACE CHECKS
    # =========================================================================
    print("\n[checking GEX Surface...]")
    gex_ref = partition_ref(cfg, "silver.future_option_mbo.gex_surface_1s", symbol, dt)
    if not gex_ref.dir.exists():
        print(f"SKIPPING: GEX partition missing at {gex_ref.dir}")
    else:
        df_gex = pd.read_parquet(gex_ref.dir)
        print(f"Loaded {len(df_gex)} rows.")

        if df_gex.empty:
            print("WARNING: GEX dataframe is empty.")
        else:
            # 1. 25 Rows per Window
            counts = df_gex.groupby("window_end_ts_ns").size()
            invalid_counts = counts[counts != 25]
            if not invalid_counts.empty:
                print(f"FAIL: Found {len(invalid_counts)} windows without exactly 25 strike rows.")
                print(invalid_counts.head())
            else:
                print("PASS: All windows have 25 strike rows.")

            # 2. Rel Ticks Alignment (Must be multiples of 20)
            # Assuming 'rel_ticks' column exists as float or int
            if "rel_ticks" in df_gex.columns:
                fails = df_gex[df_gex["rel_ticks"] % 20 != 0]
                if not fails.empty:
                    print(f"FAIL: Found {len(fails)} rows where rel_ticks is not a multiple of 20.")
                    print(fails[["window_end_ts_ns", "rel_ticks"]].head())
                else:
                    print("PASS: All rel_ticks are multiples of 20 ($5 strikes).")
                
                # Check range +/- 240
                range_fails = df_gex[df_gex["rel_ticks"].abs() > 240]
                if not range_fails.empty:
                    print(f"FAIL: Found {len(range_fails)} rows where rel_ticks > 240.")
                else:
                    print("PASS: All rel_ticks within Â±240 range.")
            else:
                print("WARNING: 'rel_ticks' column not found in GEX data.")

            # 3. Strike Monotonicity
            # Check that within each window, strike_price_int is sorted
            # This is expensive to check loop-wise.
            # Vectorized: sort by ts, strike. diff().
            df_curr = df_gex.sort_values(["window_end_ts_ns", "strike_price_int"])
            # The diff of strike_price_int should be uniform within a window
            # But between windows it might jump.
            # We can check simple uniqueness per window?
            # Or just rely on the count check.
            pass

    # =========================================================================
    # PHYSICS SURFACE CHECKS
    # =========================================================================
    print("\n[checking Physics Surface...]")
    phys_ref = partition_ref(cfg, "silver.future_mbo.physics_surface_1s", symbol, dt)
    if not phys_ref.dir.exists():
        print(f"SKIPPING: Physics partition missing at {phys_ref.dir}")
    else:
        df_phys = pd.read_parquet(phys_ref.dir)
        print(f"Loaded {len(df_phys)} rows.")
        
        if df_phys.empty:
            print("WARNING: Physics dataframe is empty.")
        else:
            # 1. Check rel_ticks exist
            if "rel_ticks" not in df_phys.columns:
                print("FAIL: rel_ticks missing in physics.")
            else:
                # 2. Check Score Bounds
                if "physics_score" in df_phys.columns:
                    bad_score = df_phys[~df_phys["physics_score"].between(0, 1)]
                    if not bad_score.empty:
                        print(f"FAIL: Found {len(bad_score)} rows with physics_score outside [0, 1].")
                    else:
                        print("PASS: physics_score in [0, 1].")
                
                if "physics_score_signed" in df_phys.columns:
                    bad_signed = df_phys[~df_phys["physics_score_signed"].between(-1, 1)]
                    if not bad_signed.empty:
                        print(f"FAIL: Found {len(bad_signed)} rows with physics_score_signed outside [-1, 1].")
                    else:
                        print("PASS: physics_score_signed in [-1, 1].")

            # 3. Check Timestamp Safety (Task 13 related)
            # Ensure no NaNs in window_end_ts_ns
            if df_phys["window_end_ts_ns"].isnull().any():
                print("FAIL: NaNs in window_end_ts_ns")
            else:
                print("PASS: Timestamps valid.")

    print("\nIntegrity V2 Complete.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dt", default="2026-01-06")
    args = parser.parse_args()
    check_integrity_v2(args.dt)
