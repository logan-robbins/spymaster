"""
Validate ES Pipeline v4.5.0 Output (Silver Layer).

This script performs structural and semantic validation of the Silver layer 'signals.parquet'
generated by the 'bronze_to_silver' pipeline (version 4.5.0).

Validation Checks:
1. File Existence: Checks for 'signals.parquet' for each date.
2. Schema Integrity:
   - Validates presence of critical column groups (Context, Dynamics, Physics, Outcomes).
   - Checks for unexpected nulls (NaNs) in critical columns.
3. Semantic Correctness:
   - Outcome Labels: Ensures valid labels (REJECT, BREAK, CHOP) and reasonable distribution.
   - Event Counts: Flags dates with unusually low/high signal counts.
   - Physics: Checks for non-zero values in physics fields (Force, Mass, Tide).

Usage:
    uv run python scripts/validate_pipeline_v45.py --start 2025-06-05 --end 2025-06-14
"""

import argparse
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from typing import Dict, List, Any

# --- Configuration ---
SILVER_VERSION = "version=4.5.0"
PIPELINE_NAME = "es_pipeline"

# Key columns that MUST exist and generally should NOT be NaN
# Based on Section A, B, D, E of EPISODE_VECTOR_SCHEMA.md
CRITICAL_COLUMNS = [
    # Context
    "minutes_since_open", "atr", "dist_to_pm_high_atr", "dist_to_ema_20_atr",
    "gamma_exposure", "gex_ratio",
    
    # Dynamics (Speed)
    "velocity_1min", "acceleration_1min", 
    "ofi_30s", "ofi_near_level_30s",
    "approach_velocity", "approach_distance_atr",
    
    # Physics (Force/Mass)
    "force_mass_ratio", "barrier_replenishment_ratio", "tape_imbalance", 
    "call_tide", "put_tide", # New in 4.5
    
    # Outcome
    "outcome", "excursion_favorable", "excursion_adverse"
]

# Columns where 0.0 is suspicious (unlikely to be exactly 0 for active markets)
# e.g. ATR should not be 0.
NON_ZERO_CHECK_COLS = [
    "atr", "velocity_1min", "force_mass_ratio"
]

EXPECTED_OUTCOMES = {"REJECT", "BREAK", "CHOP"}

def validate_date(date: str, data_root: Path) -> Dict[str, Any]:
    """Validate a single date."""
    
    # Path construction: data/silver/features/es_pipeline/version=4.5.0/date=YYYY-MM-DD/signals.parquet
    file_path = data_root / "silver" / "features" / PIPELINE_NAME / SILVER_VERSION / f"date={date}" / "signals.parquet"
    
    result = {
        "date": date,
        "status": "PASS",
        "file_exists": False,
        "row_count": 0,
        "issues": []
    }
    
    if not file_path.exists():
        result["status"] = "MISSING"
        result["issues"].append("File not found")
        return result
        
    result["file_exists"] = True
    
    try:
        df = pd.read_parquet(file_path)
        result["row_count"] = len(df)
        
        if df.empty:
            result["status"] = "FAIL"
            result["issues"].append("Empty dataframe")
            return result
            
        # 1. Schema Check
        missing_cols = [c for c in CRITICAL_COLUMNS if c not in df.columns]
        if missing_cols:
            result["status"] = "FAIL"
            result["issues"].append(f"Missing critical columns: {missing_cols}")
            # If critical cols missing, skip deeper checks for them
        
        # 2. NaN Check (Critical Cols only)
        cols_to_check = [c for c in CRITICAL_COLUMNS if c in df.columns]
        nan_counts = df[cols_to_check].isna().sum()
        cols_with_nans = nan_counts[nan_counts > 0]
        
        if not cols_with_nans.empty:
            # Check if NaNs are acceptable. 
            # In general, Silver features for a valid signal should be complete.
            # Exceptions might be certain lag features at the very start of day, but typically we filter those.
            # For now, flag as warning if low %, fail if high %.
            for col, count in cols_with_nans.items():
                pct = count / len(df)
                if pct > 0.05: # >5% nulls is bad
                    result["status"] = "FAIL" if result["status"] != "MISSING" else "MISSING"
                    result["issues"].append(f"High NaN count in {col}: {count} ({pct:.1%})")
                else:
                    result["issues"].append(f"Warning: {count} NaNs in {col}")
                    
        # 3. Physics / Zero Check
        # Check for columns that are "all zeros" (feature extraction failure)
        for col in cols_to_check:
            if pd.api.types.is_numeric_dtype(df[col]):
                if (df[col] == 0).all():
                     result["status"] = "FAIL"
                     result["issues"].append(f"Column {col} is ALL ZEROS (suspicious)")
        
        # 4. Outcome Semantics
        if "outcome" in df.columns:
            outcomes = set(df["outcome"].unique())
            invalid_outcomes = outcomes - EXPECTED_OUTCOMES
            if invalid_outcomes:
                result["status"] = "FAIL"
                result["issues"].append(f"Invalid outcomes found: {invalid_outcomes}")
            
            # Distribution check
            dist = df["outcome"].value_counts().to_dict()
            result["outcome_dist"] = dist
            
            # Simple heuristic: If we have > 10 signals, we should expect at least some mix?
            # Actually, rare days might be all REJECT. Just logging it is fine.
            
        # 5. Tide Semantics (New 4.5 Feature)
        if "call_tide" in df.columns and "put_tide" in df.columns:
            # Just ensure they aren't identical and have variance
            if df["call_tide"].equals(df["put_tide"]):
                 result["issues"].append("Warning: call_tide identical to put_tide (unlikely)")
            
    except Exception as e:
        result["status"] = "ERROR"
        result["issues"].append(f"Exception: {str(e)}")
        
    return result

def main():
    parser = argparse.ArgumentParser(description="Validate ES Pipeline v4.5.0")
    parser.add_argument("--start", type=str, required=True, help="Start Date YYYY-MM-DD")
    parser.add_argument("--end", type=str, required=True, help="End Date YYYY-MM-DD")
    parser.add_argument("--data-root", type=str, default="data", help="Data root directory")
    args = parser.parse_args()
    
    start_date = datetime.strptime(args.start, "%Y-%m-%d")
    end_date = datetime.strptime(args.end, "%Y-%m-%d")
    data_root = Path(args.data_root)
    
    print(f"\nVALIDATING PIPELINE v4.5.0 ({args.start} to {args.end})")
    print(f"Schema References: {CRITICAL_COLUMNS[:3]}...")
    print("-" * 80)
    print(f"{'DATE':<12} | {'STATUS':<8} | {'ROWS':<5} | {'OUTCOMES':<25} | {'ISSUES'}")
    print("-" * 80)
    
    curr = start_date
    metrics = {"passed": 0, "failed": 0, "missing": 0}
    
    while curr <= end_date:
        if curr.weekday() < 5: # Weekdays
            date_str = curr.strftime("%Y-%m-%d")
            res = validate_date(date_str, data_root)
            
            # Formating outcomes
            outcome_str = str(res.get("outcome_dist", ""))
            if len(outcome_str) > 25: outcome_str = outcome_str[:22] + "..."
            
            # Formatting issues - grab first one if exists
            issue_str = res["issues"][0] if res["issues"] else ""
            if len(res["issues"]) > 1: issue_str += f" (+{len(res['issues'])-1} more)"
            
            # Color/Icon
            icon = "✅" if res["status"] == "PASS" else "❌" if res["status"] == "FAIL" else "⚠️"
            
            print(f"{date_str:<12} | {icon} {res['status']:<5} | {res['row_count']:<5} | {outcome_str:<25} | {issue_str}")
            
            if res["status"] == "PASS": metrics["passed"] += 1
            elif res["status"] == "FAIL": metrics["failed"] += 1
            else: metrics["missing"] += 1
            
        curr += timedelta(days=1)

    print("-" * 80)
    print(f"SUMMARY: Passed: {metrics['passed']} | Failed: {metrics['failed']} | Missing: {metrics['missing']}")
    
    if metrics["failed"] > 0:
        exit(1)
    else:
        exit(0)

if __name__ == "__main__":
    main()
