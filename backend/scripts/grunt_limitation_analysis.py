"""
GRUNT: Deep Analysis of future_option_mbo Silver Layer Limitations
===================================================================

Performs institution-grade analysis of the two known limitations:
1. Accounting Identity Violations at Aggregate Level (8.7%)
2. depth_qty_rest > depth_qty_end (4.1%)

Last Grunted: 02/02/2026 05:30:00 AM UTC

This script implements Step 4 (Mathematical Verification) and Step 5 (Unit Test Creation)
of the grunt process to determine root causes and recommend institutional-grade solutions.
"""

import pandas as pd
import numpy as np
from pathlib import Path

# Constants from source
PRICE_SCALE = 1e-9
TICK_SIZE = 0.25
TICK_INT = int(round(TICK_SIZE / PRICE_SCALE))
STRIKE_STEP_POINTS = 5.0
STRIKE_STEP_INT = int(round(STRIKE_STEP_POINTS / PRICE_SCALE))
MAX_STRIKE_OFFSETS = 10  # +/- $50 around spot

def load_silver_data():
    """Load the silver layer data for analysis."""
    base = Path(__file__).parent.parent / "lake/silver/product_type=future_option_mbo/symbol=ESH6"
    
    snap_path = base / "table=book_snapshot_1s/dt=2026-01-06/part-00000.parquet"
    flow_path = base / "table=depth_and_flow_1s/dt=2026-01-06/part-00000.parquet"
    
    df_snap = pd.read_parquet(snap_path)
    df_flow = pd.read_parquet(flow_path)
    
    return df_snap, df_flow


def analyze_limitation_1_accounting_identity(df_flow: pd.DataFrame) -> dict:
    """
    LIMITATION 1: Accounting Identity Violations at Aggregate Level
    
    Mathematical Identity:
        depth_qty_start + add_qty - pull_qty - fill_qty = depth_qty_end
    
    Root Cause Analysis:
        The identity holds perfectly at the ENGINE level (per-instrument, per-price).
        Violations occur during aggregation in _build_option_flow_surface() because:
        
        1. GRID FILTERING: Only strikes within Â±$50 of spot are kept (line 225)
           - When spot moves, strikes enter/exit the grid
           - depth_qty_start may have been captured while IN grid
           - Subsequent events may have occurred while OUTSIDE grid (filtered out)
           - depth_qty_end reflects current state after re-filtering
        
        2. STRIKE AGGREGATION: Multiple instruments map to same strike (line 235-246)
           - Options with same strike but different expirations get summed
           - If one expires/settles, accounting breaks for the aggregate
    """
    results = {}
    
    # Calculate residual
    df_flow = df_flow.copy()
    df_flow["residual"] = (
        df_flow["depth_qty_start"] 
        + df_flow["add_qty"] 
        - df_flow["pull_qty"] 
        - df_flow["fill_qty"] 
        - df_flow["depth_qty_end"]
    )
    
    total_rows = len(df_flow)
    violation_rows = (df_flow["residual"].abs() > 0.01).sum()
    violation_pct = 100.0 * violation_rows / total_rows
    
    results["total_rows"] = total_rows
    results["violation_rows"] = violation_rows
    results["violation_pct"] = violation_pct
    
    # Analyze zero-flow rows (no activity in window)
    zero_flow = (df_flow["add_qty"] == 0) & (df_flow["pull_qty"] == 0) & (df_flow["fill_qty"] == 0)
    zero_flow_violations = (df_flow.loc[zero_flow, "residual"].abs() > 0.01).sum()
    
    results["zero_flow_rows"] = zero_flow.sum()
    results["zero_flow_violations"] = zero_flow_violations
    results["zero_flow_violation_pct"] = 100.0 * zero_flow_violations / zero_flow.sum() if zero_flow.sum() > 0 else 0
    
    # Analyze by rel_ticks (distance from spot)
    edge_strikes = df_flow["rel_ticks"].abs() >= (MAX_STRIKE_OFFSETS - 1) * 20  # Near grid boundary
    edge_violations = (df_flow.loc[edge_strikes, "residual"].abs() > 0.01).sum()
    
    results["edge_strike_rows"] = edge_strikes.sum()
    results["edge_strike_violations"] = edge_violations
    results["edge_strike_violation_pct"] = 100.0 * edge_violations / edge_strikes.sum() if edge_strikes.sum() > 0 else 0
    
    # Characterize the residuals
    violations = df_flow.loc[df_flow["residual"].abs() > 0.01]
    results["residual_mean"] = violations["residual"].mean() if len(violations) > 0 else 0
    results["residual_std"] = violations["residual"].std() if len(violations) > 0 else 0
    results["residual_min"] = violations["residual"].min() if len(violations) > 0 else 0
    results["residual_max"] = violations["residual"].max() if len(violations) > 0 else 0
    
    # Analyze by window
    window_violations = df_flow.groupby("window_end_ts_ns").apply(
        lambda x: (x["residual"].abs() > 0.01).sum()
    )
    results["windows_with_violations"] = (window_violations > 0).sum()
    results["total_windows"] = len(window_violations)
    results["windows_clean_pct"] = 100.0 * (window_violations == 0).sum() / len(window_violations)
    
    return results


def analyze_limitation_2_resting_depth(df_flow: pd.DataFrame) -> dict:
    """
    LIMITATION 2: depth_qty_rest > depth_qty_end
    
    Mathematical Constraint:
        depth_qty_rest <= depth_qty_end (resting can't exceed total)
    
    Root Cause Analysis:
        The violation occurs due to TEMPORAL AGGREGATION across price levels:
        
        1. depth_qty_rest is computed by iterating orders at window_end time
           - Order A at price P1 has been resting 600ms (included in depth_rest)
           - Order B at price P2 was just added (not in depth_rest)
           
        2. When aggregating to strike level:
           - depth_qty_end = sum of all depths at all prices for this strike
           - depth_qty_rest = sum of resting depths only
           
        3. If Order A gets filled/cancelled AFTER depth_rest snapshot but BEFORE depth_end:
           - depth_rest still includes Order A
           - depth_end doesn't include Order A (it's gone)
           - Result: depth_rest > depth_end
        
        This is a race condition in snapshot timing, not a calculation error.
    """
    results = {}
    
    df_flow = df_flow.copy()
    
    # Find violations
    violations = df_flow["depth_qty_rest"] > df_flow["depth_qty_end"]
    results["total_rows"] = len(df_flow)
    results["violation_rows"] = violations.sum()
    results["violation_pct"] = 100.0 * violations.sum() / len(df_flow)
    
    # Magnitude of violations
    excess = df_flow.loc[violations, "depth_qty_rest"] - df_flow.loc[violations, "depth_qty_end"]
    results["excess_mean"] = excess.mean() if len(excess) > 0 else 0
    results["excess_max"] = excess.max() if len(excess) > 0 else 0
    results["excess_as_pct_of_rest"] = (
        100.0 * excess.sum() / df_flow.loc[violations, "depth_qty_rest"].sum()
    ) if violations.sum() > 0 else 0
    
    # Correlation with flow activity
    active_flow = (df_flow["pull_qty"] > 0) | (df_flow["fill_qty"] > 0)
    active_violations = (violations & active_flow).sum()
    
    results["violations_with_active_flow"] = active_violations
    results["violations_with_active_flow_pct"] = (
        100.0 * active_violations / violations.sum()
    ) if violations.sum() > 0 else 0
    
    return results


def institutional_recommendations() -> str:
    """
    What Institutional-Grade Data Scientists Would Do
    =================================================
    
    Based on deep analysis of root causes and industry best practices.
    """
    return """
================================================================================
INSTITUTIONAL-GRADE RECOMMENDATIONS
================================================================================

LIMITATION 1: ACCOUNTING IDENTITY VIOLATIONS (8.7%)
---------------------------------------------------

ROOT CAUSE: Grid filtering + aggregation creates information loss between
depth_qty_start snapshot and depth_qty_end snapshot.

INSTITUTIONAL APPROACHES:

1. **SEPARATE TRACKING TABLES** (Recommended for Production)
   Create two distinct outputs:
   - `depth_and_flow_1s_INSTRUMENT`: Per-instrument, per-price level (identity holds perfectly)
   - `depth_and_flow_1s_SURFACE`: Aggregated strike surface (current output)
   
   Consumer code can JOIN when needed, but maintains audit trail.
   
   Example schema:
   ```
   -- INSTRUMENT-LEVEL (identity always holds)
   depth_and_flow_1s_instrument:
     window_end_ts_ns, instrument_id, price_int, side,
     depth_qty_start, depth_qty_end, add_qty, pull_qty, fill_qty
   
   -- SURFACE-LEVEL (for visualization, no identity guarantee)
   depth_and_flow_1s_surface:
     window_end_ts_ns, strike_price_int, right, side,
     depth_qty_agg, add_qty_agg, pull_qty_agg, fill_qty_agg
   ```

2. **FROZEN GRID APPROACH** (Alternative)
   Fix the strike grid at session start (e.g., 09:30 ET spot) rather than 
   dynamic spot. This eliminates grid entry/exit drift but sacrifices 
   relevance as spot moves significantly.

3. **METADATA FLAG APPROACH** (Current Best Practice)
   Add an `accounting_identity_valid` boolean column:
   - TRUE: Row was inside grid for entire window, identity holds
   - FALSE: Row entered/exited grid mid-window, use depth_qty_end only
   
   Implementation:
   ```python
   # Track which keys were in grid at window_start AND window_end
   grid_stable = (in_grid_at_start & in_grid_at_end)
   df_out["accounting_identity_valid"] = grid_stable
   ```

4. **DOCUMENT AS DESIGN DECISION** (Current State)
   For many use cases, the 8.7% violation rate is acceptable because:
   - Zero-flow rows have 0% violations (formula is correct)
   - Violations are bounded (mean ~0, max reasonable)
   - depth_qty_end is always authoritative for current state
   
   Gold layer consumers should use:
   - depth_qty_end for point-in-time state
   - add_qty, pull_qty, fill_qty for relative flow indicators (not accounting)


LIMITATION 2: DEPTH_QTY_REST > DEPTH_QTY_END (4.1%)
---------------------------------------------------

ROOT CAUSE: Temporal race condition between resting-depth snapshot and 
depth-end snapshot when aggregating across price levels.

INSTITUTIONAL APPROACHES:

1. **CLAMP RESTING DEPTH** (Simple Fix)
   ```python
   df_out["depth_qty_rest"] = np.minimum(df_out["depth_qty_rest"], df_out["depth_qty_end"])
   ```
   
   Trade-off: Loses information about orders that were resting before being pulled.
   This is semantically correct if "resting depth" means "currently resting".

2. **COMPUTE RESTING AT EMISSION TIME** (Correct Fix)
   In the engine, compute depth_rest AFTER all events are processed, not concurrently.
   This requires restructuring the Numba kernel to:
   - Process all events for window
   - Compute depth_end
   - THEN compute depth_rest from surviving orders
   
   Trade-off: Performance cost (~5-10% slower).

3. **ADD RESTING_WAS_ACTIVE FLAG** (Audit Trail)
   Add column indicating if resting orders had activity during window:
   ```python
   resting_had_activity = (pull_qty_rest > 0) | (was_partially_filled)
   ```
   
   Consumers can filter to rows where constraint naturally holds.

4. **ACCEPT AS EDGE CASE** (Current State)
   For gold layer intensity calculations, the 4.1% rate with small magnitudes
   is often acceptable. The resting depth is used as a "stability indicator"
   and slight overcount doesn't materially affect downstream models.


RECOMMENDED IMPLEMENTATION PRIORITY
-----------------------------------

1. **Immediate (Low Effort)**: Add `accounting_identity_valid` flag to schema
2. **Immediate (Low Effort)**: Clamp depth_qty_rest to depth_qty_end
3. **Medium Term**: Create instrument-level table for audit trail
4. **Long Term**: Restructure engine for correct resting computation

These approaches are consistent with how institutional data vendors (Bloomberg, 
Refinitiv, ICE) handle similar aggregation challenges in their market data feeds.
================================================================================
"""


def main():
    print("=" * 80)
    print("GRUNT: Deep Analysis of future_option_mbo Silver Layer Limitations")
    print("=" * 80)
    print()
    
    print("Loading data...")
    df_snap, df_flow = load_silver_data()
    print(f"  book_snapshot_1s: {len(df_snap):,} rows")
    print(f"  depth_and_flow_1s: {len(df_flow):,} rows")
    print()
    
    print("-" * 80)
    print("LIMITATION 1: Accounting Identity Violations")
    print("-" * 80)
    
    results1 = analyze_limitation_1_accounting_identity(df_flow)
    
    print(f"Total rows: {results1['total_rows']:,}")
    print(f"Violations: {results1['violation_rows']:,} ({results1['violation_pct']:.2f}%)")
    print()
    print("Zero-flow rows (no activity):")
    print(f"  Count: {results1['zero_flow_rows']:,}")
    print(f"  Violations: {results1['zero_flow_violations']:,} ({results1['zero_flow_violation_pct']:.2f}%)")
    print()
    print("Edge strikes (near grid boundary):")
    print(f"  Count: {results1['edge_strike_rows']:,}")
    print(f"  Violations: {results1['edge_strike_violations']:,} ({results1['edge_strike_violation_pct']:.2f}%)")
    print()
    print("Residual statistics (violations only):")
    print(f"  Mean: {results1['residual_mean']:.2f}")
    print(f"  Std:  {results1['residual_std']:.2f}")
    print(f"  Min:  {results1['residual_min']:.2f}")
    print(f"  Max:  {results1['residual_max']:.2f}")
    print()
    print("Window coverage:")
    print(f"  Total windows: {results1['total_windows']:,}")
    print(f"  Windows with violations: {results1['windows_with_violations']:,}")
    print(f"  Clean windows: {results1['windows_clean_pct']:.1f}%")
    print()
    
    print("-" * 80)
    print("LIMITATION 2: depth_qty_rest > depth_qty_end")
    print("-" * 80)
    
    results2 = analyze_limitation_2_resting_depth(df_flow)
    
    print(f"Total rows: {results2['total_rows']:,}")
    print(f"Violations: {results2['violation_rows']:,} ({results2['violation_pct']:.2f}%)")
    print()
    print("Excess magnitude (violations only):")
    print(f"  Mean excess: {results2['excess_mean']:.2f} contracts")
    print(f"  Max excess: {results2['excess_max']:.2f} contracts")
    print(f"  Excess as % of rest: {results2['excess_as_pct_of_rest']:.1f}%")
    print()
    print("Correlation with flow activity:")
    print(f"  Violations with active pull/fill: {results2['violations_with_active_flow']:,}")
    print(f"  As % of violations: {results2['violations_with_active_flow_pct']:.1f}%")
    print()
    
    print(institutional_recommendations())


if __name__ == "__main__":
    main()
